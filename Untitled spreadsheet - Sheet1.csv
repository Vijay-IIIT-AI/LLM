Model Name,Model size (params),"RAM (weights, FP16 ≈ 2 B/param)","KV cache per 1,000 tokens (FP16, approx)"
Llama-3.2-8B-Instruct,~8B,~16 GB,≈ 0.5 GB
Qwen2.8-7B-Instruct (7B),~7B,~14 GB,≈ 0.5 GB
Florence-2-large (0.77B),~0.77B,~1.5 GB,≈ 0.19 GB (≈ 190 MB)
Llama-3.2-11B-Vision_Instruct,~10.6–11B,~22 GB,≈ 0.94 GB
Gemma-3-27B-it,~27B,~54 GB,≈ 1.4 GB
Qwen2.5-VL-72B-Instruct,~72B,~144 GB,≈ 3.8 GB
Llama-3.2-90B-Vision-Instruct,~88–90B,~180 GB,≈ 3.8 GB