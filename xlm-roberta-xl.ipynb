{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:48:40.078691Z","iopub.execute_input":"2024-12-03T04:48:40.081975Z","iopub.status.idle":"2024-12-03T04:48:41.256878Z","shell.execute_reply.started":"2024-12-03T04:48:40.081913Z","shell.execute_reply":"2024-12-03T04:48:41.255687Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/xlm-roberta-xl\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"facebook/xlm-roberta-xl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:48:59.638418Z","iopub.execute_input":"2024-12-03T04:48:59.639247Z","iopub.status.idle":"2024-12-03T04:50:41.368537Z","shell.execute_reply.started":"2024-12-03T04:48:59.639206Z","shell.execute_reply":"2024-12-03T04:50:41.367338Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"287067e6d9a3461e9ed07847e785bb74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f14ad11821134fc29b1486a29d1c3036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e2c405711354cfea0d22932acfef0ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d033b4801a4641ff87a1e1fb44bee443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ac88c2387749b5a1c98614cf5f35a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/13.9G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73431721f6c3469dbd195a406ea9eda9"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:51:55.648240Z","iopub.execute_input":"2024-12-03T04:51:55.648664Z","iopub.status.idle":"2024-12-03T04:51:55.657892Z","shell.execute_reply.started":"2024-12-03T04:51:55.648629Z","shell.execute_reply":"2024-12-03T04:51:55.656945Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"XLMRobertaXLForMaskedLM(\n  (roberta): XLMRobertaXLModel(\n    (embeddings): XLMRobertaXLEmbeddings(\n      (word_embeddings): Embedding(250880, 2560, padding_idx=1)\n      (position_embeddings): Embedding(514, 2560, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 2560)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaXLEncoder(\n      (layer): ModuleList(\n        (0-35): 36 x XLMRobertaXLLayer(\n          (attention): XLMRobertaXLAttention(\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (self): XLMRobertaXLSdpaSelfAttention(\n              (query): Linear(in_features=2560, out_features=2560, bias=True)\n              (key): Linear(in_features=2560, out_features=2560, bias=True)\n              (value): Linear(in_features=2560, out_features=2560, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaXLSelfOutput(\n              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaXLIntermediate(\n            (dense): Linear(in_features=2560, out_features=10240, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaXLOutput(\n            (dense): Linear(in_features=10240, out_features=2560, bias=True)\n          )\n          (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): XLMRobertaXLLMHead(\n    (dense): Linear(in_features=2560, out_features=2560, bias=True)\n    (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=2560, out_features=250880, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:52:08.749481Z","iopub.execute_input":"2024-12-03T04:52:08.749895Z","iopub.status.idle":"2024-12-03T04:52:08.761967Z","shell.execute_reply.started":"2024-12-03T04:52:08.749864Z","shell.execute_reply":"2024-12-03T04:52:08.760899Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"XLMRobertaXLForMaskedLM(\n  (roberta): XLMRobertaXLModel(\n    (embeddings): XLMRobertaXLEmbeddings(\n      (word_embeddings): Embedding(250880, 2560, padding_idx=1)\n      (position_embeddings): Embedding(514, 2560, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 2560)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaXLEncoder(\n      (layer): ModuleList(\n        (0-35): 36 x XLMRobertaXLLayer(\n          (attention): XLMRobertaXLAttention(\n            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n            (self): XLMRobertaXLSdpaSelfAttention(\n              (query): Linear(in_features=2560, out_features=2560, bias=True)\n              (key): Linear(in_features=2560, out_features=2560, bias=True)\n              (value): Linear(in_features=2560, out_features=2560, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaXLSelfOutput(\n              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaXLIntermediate(\n            (dense): Linear(in_features=2560, out_features=10240, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaXLOutput(\n            (dense): Linear(in_features=10240, out_features=2560, bias=True)\n          )\n          (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (LayerNorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): XLMRobertaXLLMHead(\n    (dense): Linear(in_features=2560, out_features=2560, bias=True)\n    (layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=2560, out_features=250880, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Define your sentences\nsentence_en = \"It is hot today.\"\nsentence_ko = \"이것은 한국어로 된 예문입니다.\"\n\n# Function to encode sentences and get embeddings\ndef get_embedding(sentence):\n    # Tokenize the sentence\n    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    \n    # Get the model output\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Extract the last hidden state\n    # We can use the output of the [CLS] token as the sentence embedding\n    embeddings = outputs[0][:, 0, :]  # [CLS] token representation\n    return embeddings\n\n# Get embeddings for both sentences\nembedding_en = get_embedding(sentence_en)\nembedding_ko = get_embedding(sentence_ko)\n\n# Compute cosine similarity\nsimilarity = cosine_similarity(embedding_en.numpy(), embedding_ko.numpy())\n\nprint(\"Cosine Similarity:\", similarity[0][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:55:10.544063Z","iopub.execute_input":"2024-12-03T04:55:10.544804Z","iopub.status.idle":"2024-12-03T04:55:15.006144Z","shell.execute_reply.started":"2024-12-03T04:55:10.544768Z","shell.execute_reply":"2024-12-03T04:55:15.004986Z"}},"outputs":[{"name":"stdout","text":"Cosine Similarity: 0.99751174\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\n\n\ndf = pd.DataFrame({\n    'eng_sent': [sentence_en],\n    'kor_sent': [sentence_ko]\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:58:02.181329Z","iopub.execute_input":"2024-12-03T04:58:02.182068Z","iopub.status.idle":"2024-12-03T04:58:02.188441Z","shell.execute_reply.started":"2024-12-03T04:58:02.182031Z","shell.execute_reply":"2024-12-03T04:58:02.187236Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport time\n\nmodels = [\n    \n    \"xlm-roberta-base\",\n    \"FacebookAI/xlm-roberta-base\",\n    'paraphrase-multilingual-mpnet-base-v2',\n    \"paraphrase-multilingual-MiniLM-L12-v2\"\n]\n\n\n# List of free models\n# models = [\n    \n#     \"xlm-roberta-base\"\n# ]\n\n# Dictionaries to store results\nsimilarity_results = {model_name: [] for model_name in models}\ntime_results = {model_name: 0 for model_name in models}\n\nfor model_name in models:\n    print(\"-------------------------------------------------\")\n    print(f\"Processing model: {model_name}\")\n    \n    start_time = time.time()  # Start timer\n    \n    try:\n        # Load model\n        model = SentenceTransformer(model_name, trust_remote_code=True)\n        \n        # Encode sentences\n        kor_embeddings = model.encode(df['kor_sent'].tolist())\n        eng_embeddings = model.encode(df['eng_sent'].tolist())\n        \n        # Compute similarity scores\n        similarity_scores = [cosine_similarity([kor_embeddings[i]], [eng_embeddings[i]])[0, 0] for i in range(len(df))]\n        \n        # Store similarity scores\n        similarity_results[model_name] = similarity_scores\n    except Exception as e:\n        print(f\"Failed to process model {model_name}: {e}\")\n        similarity_results[model_name] = [None] * len(df)\n    finally:\n        end_time = time.time()  # End timer\n        time_taken = end_time - start_time\n        time_results[model_name] = time_taken\n        print(f\"Time taken: {time_taken:.2f} seconds\")\n         # Clear the model from memory\n        #del model\n        #gc.collect()  # Force garbage collection\n\n# Add similarities and time taken as columns to the DataFrame\nfor model_name, scores in similarity_results.items():\n    df[model_name + \"_sim\"] = scores\n\nfor model_name, time_taken in time_results.items():\n    df[model_name + \"_time\"] = time_taken\n\n# Find the model with the highest similarity for each row\ndf[\"best_model\"] = df[[col for col in df.columns if col.endswith(\"_sim\")]].idxmax(axis=1)\n\n# Clean the column names in `best_model`\ndf[\"best_model\"] = df[\"best_model\"].str.replace(\"_sim\", \"\")\n\n# Add the time taken for the best model\ndf[\"model_time_taken\"] = df[\"best_model\"].map(time_results)\n\n# Add optimal time taken (minimum across all models)\ndf[\"optimal_time_taken\"] = min(time_results.values())\n\nprint(df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:58:46.066433Z","iopub.execute_input":"2024-12-03T04:58:46.067445Z","iopub.status.idle":"2024-12-03T05:01:17.207246Z","shell.execute_reply.started":"2024-12-03T04:58:46.067403Z","shell.execute_reply":"2024-12-03T05:01:17.206287Z"}},"outputs":[{"name":"stdout","text":"-------------------------------------------------\nProcessing model: xlm-roberta-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9eb3c2643e4c3181009ae5fc7b8b50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ab38ecb73d43f8938ad2d9b895feeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f921dd3b4b64040b836732219ce55cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c804ba1239b3434cb43ec00052b69248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d096600ce0494a7a9c5c98f859083222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c71327abef194927b33d1803dab69dea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b2fec5e27e4ce59a62942aef320d65"}},"metadata":{}},{"name":"stdout","text":"Time taken: 50.01 seconds\n-------------------------------------------------\nProcessing model: FacebookAI/xlm-roberta-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb1e85a6ef74772ac09845b8a767bc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d7ceea1574d40e6825d77e23a3db4e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dddbea0aac94ba887fd60f9ce7472d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbce9396218b4f87a862ed841feafd32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e2d4d5883047ad9282ee1b3215120d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14bd4fd6c7fe4b0eaae306a869937ac7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"100f0b2692114b45ace9860398d542c1"}},"metadata":{}},{"name":"stdout","text":"Time taken: 43.27 seconds\n-------------------------------------------------\nProcessing model: paraphrase-multilingual-mpnet-base-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2572739b4d943ee9300d035d76c8b46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78aab6a8e38046639182e13bbcc5e6cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded175a879c24d3b8e636f052ffb1adb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96eb7dc896b4fc39b5e03be631d17f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a813f98e304aed99fb5fcec727f609"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae649fcf70b43cf933c6ce8c0674170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2996cf48dcba4b66aeb36a997d7b7990"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4fea45b3dc54c37a3d14e9b9d157164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9377927834d54178a2fd7171ecddb306"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0336cb17a41145a8a2b6db7b659b415f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"393f9d7fd8ae4716af128d7b5d455ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b017076726da4963ba54dcfb17d23cef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ffb5fae1314b2a8c0bbb1f4b9e93eb"}},"metadata":{}},{"name":"stdout","text":"Time taken: 20.72 seconds\n-------------------------------------------------\nProcessing model: paraphrase-multilingual-MiniLM-L12-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c1b3491f4840b7ae4e6d5668f622e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"042da1eba05542f4b13a9731a360adff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60341827ac864e64a4b8fab90a29a251"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"249fc23861ac473db3b09ab42ae4df56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb766d76b854e45a8b5bfa29ff04fa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b76d666545649a0abbab11a93a68392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e210a2b99d4214b61533a86a070610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"955b1c33130b4cb3a2a3cec08864b7a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1bbab852e741249f30ea7afe7d34c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5498bb39a5bc4b1385d1fd287d13d473"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ff8867fa5ba435992a6446996c1f035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5735eb3b616437faa83066c6458d8a5"}},"metadata":{}},{"name":"stdout","text":"Time taken: 35.28 seconds\n           eng_sent           kor_sent  xlm-roberta-base_sim  \\\n0  It is hot today.  이것은 한국어로 된 예문입니다.              0.993354   \n\n   FacebookAI/xlm-roberta-base_sim  paraphrase-multilingual-mpnet-base-v2_sim  \\\n0                         0.993354                                   0.232731   \n\n   paraphrase-multilingual-MiniLM-L12-v2_sim  xlm-roberta-base_time  \\\n0                                    0.07898              50.010739   \n\n   FacebookAI/xlm-roberta-base_time  \\\n0                         43.274361   \n\n   paraphrase-multilingual-mpnet-base-v2_time  \\\n0                                   20.721071   \n\n   paraphrase-multilingual-MiniLM-L12-v2_time        best_model  \\\n0                                   35.284058  xlm-roberta-base   \n\n   model_time_taken  optimal_time_taken  \n0         50.010739           20.721071  \n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:01:18.185954Z","iopub.execute_input":"2024-12-03T05:01:18.186316Z","iopub.status.idle":"2024-12-03T05:01:18.205168Z","shell.execute_reply.started":"2024-12-03T05:01:18.186270Z","shell.execute_reply":"2024-12-03T05:01:18.203626Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"           eng_sent           kor_sent  xlm-roberta-base_sim  \\\n0  It is hot today.  이것은 한국어로 된 예문입니다.              0.993354   \n\n   FacebookAI/xlm-roberta-base_sim  paraphrase-multilingual-mpnet-base-v2_sim  \\\n0                         0.993354                                   0.232731   \n\n   paraphrase-multilingual-MiniLM-L12-v2_sim  xlm-roberta-base_time  \\\n0                                    0.07898              50.010739   \n\n   FacebookAI/xlm-roberta-base_time  \\\n0                         43.274361   \n\n   paraphrase-multilingual-mpnet-base-v2_time  \\\n0                                   20.721071   \n\n   paraphrase-multilingual-MiniLM-L12-v2_time        best_model  \\\n0                                   35.284058  xlm-roberta-base   \n\n   model_time_taken  optimal_time_taken  \n0         50.010739           20.721071  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eng_sent</th>\n      <th>kor_sent</th>\n      <th>xlm-roberta-base_sim</th>\n      <th>FacebookAI/xlm-roberta-base_sim</th>\n      <th>paraphrase-multilingual-mpnet-base-v2_sim</th>\n      <th>paraphrase-multilingual-MiniLM-L12-v2_sim</th>\n      <th>xlm-roberta-base_time</th>\n      <th>FacebookAI/xlm-roberta-base_time</th>\n      <th>paraphrase-multilingual-mpnet-base-v2_time</th>\n      <th>paraphrase-multilingual-MiniLM-L12-v2_time</th>\n      <th>best_model</th>\n      <th>model_time_taken</th>\n      <th>optimal_time_taken</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>It is hot today.</td>\n      <td>이것은 한국어로 된 예문입니다.</td>\n      <td>0.993354</td>\n      <td>0.993354</td>\n      <td>0.232731</td>\n      <td>0.07898</td>\n      <td>50.010739</td>\n      <td>43.274361</td>\n      <td>20.721071</td>\n      <td>35.284058</td>\n      <td>xlm-roberta-base</td>\n      <td>50.010739</td>\n      <td>20.721071</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:58:30.129417Z","iopub.execute_input":"2024-12-03T04:58:30.129821Z","iopub.status.idle":"2024-12-03T04:58:41.789176Z","shell.execute_reply.started":"2024-12-03T04:58:30.129789Z","shell.execute_reply":"2024-12-03T04:58:41.788107Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}